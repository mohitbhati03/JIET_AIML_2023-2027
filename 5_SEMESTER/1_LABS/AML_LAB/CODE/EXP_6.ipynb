{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf8ea69-66e8-4543-9855-93dcd3f309f1",
   "metadata": {},
   "source": [
    "Objective: To evaluate classifiers using baseline methods: constant, uniform, stratified, prior, and most frequent on the Wine dataset and find the accuracy. Identify the patterns using ROC and AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc5cd9-bc6f-4ee9-b8dd-1a7df19b58cc",
   "metadata": {},
   "source": [
    "Theory:\n",
    "\n",
    "### Classification Evaluation and Baseline Models\n",
    "\n",
    "Classification evaluation involves assessing how well a model predicts categorical outcomes. Key metrics include accuracy, precision, recall, F1-score, and AUC. Baseline classifiers serve as reference models that do not learn from data but follow simple strategies:\n",
    "\n",
    "| Strategy        | Description                                                                 |\n",
    "|----------------|-------------------------------------------------------------------------------|\n",
    "| constant        | Predicts a fixed class label for all inputs.                                |\n",
    "| uniform         | Predicts classes randomly with equal probability.                           |\n",
    "| stratified      | Predicts classes based on training set distribution.                        |\n",
    "| prior           | Predicts based on prior class probabilities.                                |\n",
    "| most_frequent   | Always predicts the most common class in the training set.                  |\n",
    "\n",
    "These models help determine whether a trained classifier performs better than naive approaches.\n",
    "\n",
    "### ROC and AUC in Multi-Class Settings\n",
    "\n",
    "ROC curves plot the True Positive Rate vs. False Positive Rate. AUC measures the area under this curve, indicating model discrimination ability. For multi-class problems, ROC/AUC can be computed using one-vs-rest or macro/micro averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0709c9b2-1805-4f11-9f73-d652881e0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991e5412-1b0e-4e5b-b66d-e0e79de9001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "print(X)\n",
    "print(y)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07da3c0b-38d6-4a8c-832e-9fe473951dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'constant': DummyClassifier(constant=0, random_state=42, strategy='constant'), 'uniform': DummyClassifier(constant=0, random_state=42, strategy='uniform'), 'stratified': DummyClassifier(constant=0, random_state=42, strategy='stratified'), 'prior': DummyClassifier(constant=0, random_state=42), 'most_frequent': DummyClassifier(constant=0, random_state=42, strategy='most_frequent')}\n"
     ]
    }
   ],
   "source": [
    "# Initialize baseline classifiers\n",
    "strategies = ['constant', 'uniform', 'stratified', 'prior', 'most_frequent']\n",
    "constant_value = 0  # for 'constant' strategy\n",
    "classifiers = {strategy: DummyClassifier(strategy=strategy, random_state=42, constant=constant_value) for strategy in strategies}\n",
    "print(classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6b6cef-436b-4be0-8f8e-24501f553b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using constant strategy: 0.3519\n",
      "Accuracy using uniform strategy: 0.3333\n",
      "Accuracy using stratified strategy: 0.3704\n",
      "Accuracy using prior strategy: 0.3889\n",
      "Accuracy using most_frequent strategy: 0.3889\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each classifier\n",
    "accuracies = {}\n",
    "for strategy, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies[strategy] = accuracy\n",
    "    print(f'Accuracy using {strategy} strategy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9a310-8855-4123-a302-a049f98e1782",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifiers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve, auc\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m strategy, clf \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclassifiers\u001b[49m.items():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(clf, \u001b[33m\"\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m\"\u001b[39m):  \n\u001b[32m      5\u001b[39m         y_prob = clf.predict_proba(X_test)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'classifiers' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate ROC curves and calculate AUC\n",
    "for strategy, clf in classifiers.items():\n",
    "    if strategy != 'constant':  # 'constant' strategy doesn't work with roc_curve\n",
    "        y_prob = clf.predict_proba(X_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1], pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{strategy} (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86a4c3-1bec-4a95-9308-b5414a005c95",
   "metadata": {},
   "source": [
    "Result:\n",
    "| Strategy        | Accuracy | AUC (if applicable) | Remarks                            |\n",
    "|----------------|----------|---------------------|-------------------------------------|\n",
    "| constant        | ~0.35    | N/A                 | Predicts one class; not ROC-eligible |\n",
    "| uniform         | ~0.33    | Low                 | Random guessing baseline            |\n",
    "| stratified      | ~0.37    | Moderate            | Preserves class distribution        |\n",
    "| prior           | ~0.38    | Moderate            | Based on training class frequencies |\n",
    "| most_frequent   | ~0.38   | Low                 | May outperform random if imbalance exists |\n",
    "\n",
    "**Observation:** All baseline classifiers perform near chance level. Most_frequent may appear stronger due to class imbalance but lacks generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66912aa-f5e6-415f-b132-72576727a3dc",
   "metadata": {},
   "source": [
    "Conclusion: \n",
    "Baseline classifiers offer a benchmark for evaluating model performance. They highlight whether a trained model truly learns patterns or merely reflects data biases. While accuracy is useful, multi-class problems require deeper metrics like macro/micro-averaged AUC. This practical emphasizes the importance of thoughtful evaluation and metric selection in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f652c-a741-42ab-aa20-73f393a39509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
