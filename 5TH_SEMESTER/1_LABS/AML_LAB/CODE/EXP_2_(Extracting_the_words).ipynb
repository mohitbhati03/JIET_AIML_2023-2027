{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "576df3c3-53a4-40dc-b8e2-2fb2bc730dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Objective:\\nTo implement the operation of extracting the words (features) used in a sentence.\\nTheory:\\nText Pre-Processing\\nBefore feature engineering, we need to pre-process, clean, and normalize the text like we mentioned before. There are multiple pre-processing techniques, some of which are quite elaborate. We will not be going into a lot of details in this section but we will be covering a lot of them in further detail in a future chapter when we work on text classification and sentiment analysis. Following is some of the popular pre-processing techniques.\\n• Text tokenization and lower casing\\n• Removing special characters\\n• Contraction expansion\\n• Removing stopwords\\n• Correcting spellings\\n• Stemming\\n• Lemmatization\\nSome important terms:\\nTokenization: Splitting of string data into constituent units. For example, splitting sentences into words or words into characters.\\nStemming and lemmatization: These are normalization methods to bring words into their root or canonical forms. While stemming is a heuristic process to achieve the root form, lemmatization utilizes rules of grammar and vocabulary to derive the root.\\nStopword Removal: Text contains words that occur at high frequency yet do not convey much information (punctuations, conjunctions, and so on). These words/phrases are usually removed to reduce dimensionality and noise from data.\\nCorpora: The starting point of any text analytics process is the process of collecting the documents of interest in a single dataset. This dataset is central to the next steps of processing and analysis. This collection of documents is generally called a corpus. Multiple corpus datasets are called corpora. \\nTagging: The process of tagging will involve getting a text corpus, tokenizing the text and assigning metadata information like tags to each word in the corpora.\\nChunking: Chunking is a process which is similar to parsing or tokenization but the major difference is that instead of trying to parse each word, we will target phrases present in the document. \\n\\nPrerequisites:\\n•\\tBasic knowledge of Python programming\\n•\\tFamiliarity with string operations\\nRequired Tools:\\n•\\tPython installed on computer\\n\\nMethod 1: Without using any dedicated library\\nSteps:\\nSteps 1: Define the Punctuation Characters\\nA set of punctuation characters that should be removed from the sentence with be define.\\nStep 2: Normalize the Sentence\\nThe sentence will be converted to lowercase to standardize the words.\\nStep 3: Remove Punctuation\\nAny punctuation characters will be removed from the sentence.\\nStep 4: Tokenize the Sentence\\nThe cleaned sentence will be split into individual words.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Objective:\n",
    "To implement the operation of extracting the words (features) used in a sentence.\n",
    "Theory:\n",
    "Text Pre-Processing\n",
    "Before feature engineering, we need to pre-process, clean, and normalize the text like we mentioned before. There are multiple pre-processing techniques, some of which are quite elaborate. We will not be going into a lot of details in this section but we will be covering a lot of them in further detail in a future chapter when we work on text classification and sentiment analysis. Following is some of the popular pre-processing techniques.\n",
    "• Text tokenization and lower casing\n",
    "• Removing special characters\n",
    "• Contraction expansion\n",
    "• Removing stopwords\n",
    "• Correcting spellings\n",
    "• Stemming\n",
    "• Lemmatization\n",
    "Some important terms:\n",
    "Tokenization: Splitting of string data into constituent units. For example, splitting sentences into words or words into characters.\n",
    "Stemming and lemmatization: These are normalization methods to bring words into their root or canonical forms. While stemming is a heuristic process to achieve the root form, lemmatization utilizes rules of grammar and vocabulary to derive the root.\n",
    "Stopword Removal: Text contains words that occur at high frequency yet do not convey much information (punctuations, conjunctions, and so on). These words/phrases are usually removed to reduce dimensionality and noise from data.\n",
    "Corpora: The starting point of any text analytics process is the process of collecting the documents of interest in a single dataset. This dataset is central to the next steps of processing and analysis. This collection of documents is generally called a corpus. Multiple corpus datasets are called corpora. \n",
    "Tagging: The process of tagging will involve getting a text corpus, tokenizing the text and assigning metadata information like tags to each word in the corpora.\n",
    "Chunking: Chunking is a process which is similar to parsing or tokenization but the major difference is that instead of trying to parse each word, we will target phrases present in the document. \n",
    "\n",
    "Prerequisites:\n",
    "•\tBasic knowledge of Python programming\n",
    "•\tFamiliarity with string operations\n",
    "Required Tools:\n",
    "•\tPython installed on computer\n",
    "\n",
    "Method 1: Without using any dedicated library\n",
    "Steps:\n",
    "Steps 1: Define the Punctuation Characters\n",
    "A set of punctuation characters that should be removed from the sentence with be define.\n",
    "Step 2: Normalize the Sentence\n",
    "The sentence will be converted to lowercase to standardize the words.\n",
    "Step 3: Remove Punctuation\n",
    "Any punctuation characters will be removed from the sentence.\n",
    "Step 4: Tokenize the Sentence\n",
    "The cleaned sentence will be split into individual words.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3528aab-f662-4b01-9ad0-647b6e758a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The stating point of any text analytics process is the process of collecting the documents of interest in a single dataset. This dataset is central to the next steps of processing and analysis .This collection of documents is generally called a corpus. Multiple corpus darasets are called corpora.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acf0e67e-2325-4a37-8dd5-35c77a498319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\;'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\;'\n",
      "C:\\Users\\jalen\\AppData\\Local\\Temp\\ipykernel_5500\\2204921589.py:3: SyntaxWarning: invalid escape sequence '\\;'\n",
      "  punctuation = '''~!`!@#$%^&*()_-+=[{]}|'\"\\;:,<.>/?'''\n"
     ]
    }
   ],
   "source": [
    "def extract_words(sentence):\n",
    "    #Define a set of punctuation characters to remove.\n",
    "    punctuation = '''~!`!@#$%^&*()_-+=[{]}|'\"\\;:,<.>/?'''\n",
    "    #Normalize the sentence\n",
    "    sentence = sentence.lower() #Convert to lowercase\n",
    "    #Remove punctuation\n",
    "    cleaned_sentence = ''\n",
    "    for char in sentence:\n",
    "        if char not in punctuation:\n",
    "            cleaned_sentence += char\n",
    "    words = cleaned_sentence.split()\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0d3119a-ff65-45b2-940a-1ce4cd6fdc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrected words: ['the', 'stating', 'point', 'of', 'any', 'text', 'analytics', 'process', 'is', 'the', 'process', 'of', 'collecting', 'the', 'documents', 'of', 'interest', 'in', 'a', 'single', 'dataset', 'this', 'dataset', 'is', 'central', 'to', 'the', 'next', 'steps', 'of', 'processing', 'and', 'analysis', 'this', 'collection', 'of', 'documents', 'is', 'generally', 'called', 'a', 'corpus', 'multiple', 'corpus', 'darasets', 'are', 'called', 'corpora']\n"
     ]
    }
   ],
   "source": [
    "words = extract_words(sentence)\n",
    "print('Extrected words:', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01430370-a8e2-4657-827c-658670fba50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jalen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jalen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Method 2: Using re and nltk libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re #regular expression\n",
    "import nltk  #natural language toolkit\n",
    "\n",
    "#Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46bf38cf-fe10-4caa-a989-251ec3fdf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Load Data\n",
    "corpus = ['The sky is blue and beautiful.',\n",
    "'Love this blue and beautiful sky!',\n",
    "'The quick brown fox jumps over the lazy dog.',\n",
    "'The brown fox is quick and the blue dog is lazy!',\n",
    "'The sky is very blue and the sky is very beautiful today',\n",
    "'The dog is lazy but the brown fox is quick!']\n",
    "\n",
    "labels = ['weather','weather', 'animals', 'animals', 'weather','animals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6223bf1c-cd70-4dc2-94c5-626be5e7f582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus:\n",
      "                                            Document Category\n",
      "0                     The sky is blue and beautiful.  weather\n",
      "1                  Love this blue and beautiful sky!  weather\n",
      "2       The quick brown fox jumps over the lazy dog.  animals\n",
      "3   The brown fox is quick and the blue dog is lazy!  animals\n",
      "4  The sky is very blue and the sky is very beaut...  weather\n",
      "5        The dog is lazy but the brown fox is quick!  animals\n"
     ]
    }
   ],
   "source": [
    "#Create a DataFrame\n",
    "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
    "print('Original Corpus:')\n",
    "print(corpus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70da3d06-606b-40eb-bd02-b32b6ee48e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e40bbedd-2585-4cce-8dd3-bdc1aba7e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the text\n",
    "def normalize_document(doc):\n",
    "    # Remove special characters and lowercase\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]','', doc)   #re.sub(pattern, replacement, string, flag)\n",
    "    doc = doc.lower().strip()\n",
    "    # Tokenize\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    #Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    #Reconstruct document\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c6e5c8d-c33d-44bc-b5f1-1bb5d3ef84a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized Corpus:\n",
      "              Normalized_Document Category\n",
      "0              sky blue beautiful  weather\n",
      "1         love blue beautiful sky  weather\n",
      "2  quick brown fox jumps lazy dog  animals\n",
      "3   brown fox quick blue dog lazy  animals\n",
      "4    sky blue sky beautiful today  weather\n",
      "5        dog lazy brown fox quick  animals\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "normalize_corpus\n",
    "corpus_df['Normalized_Document'] = normalize_corpus(corpus_df['Document'])\n",
    "\n",
    "print(\"\\nNormalized Corpus:\")\n",
    "print(corpus_df[['Normalized_Document', 'Category']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e4155-008c-435c-a063-4eb60babdd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
